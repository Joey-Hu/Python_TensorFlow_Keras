{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Iter0,Testing Acc 0.9253,learning_rate 0.001\n",
      "Iter1,Testing Acc 0.9445,learning_rate 0.00095\n",
      "Iter2,Testing Acc 0.9552,learning_rate 0.0009025\n",
      "Iter3,Testing Acc 0.9606,learning_rate 0.000857375\n",
      "Iter4,Testing Acc 0.965,learning_rate 0.00081450626\n",
      "Iter5,Testing Acc 0.9679,learning_rate 0.0007737809\n",
      "Iter6,Testing Acc 0.9674,learning_rate 0.0007350919\n",
      "Iter7,Testing Acc 0.9677,learning_rate 0.0006983373\n",
      "Iter8,Testing Acc 0.97,learning_rate 0.0006634204\n",
      "Iter9,Testing Acc 0.9707,learning_rate 0.0006302494\n",
      "Iter10,Testing Acc 0.973,learning_rate 0.0005987369\n",
      "Iter11,Testing Acc 0.9722,learning_rate 0.0005688001\n",
      "Iter12,Testing Acc 0.9714,learning_rate 0.0005403601\n",
      "Iter13,Testing Acc 0.9726,learning_rate 0.0005133421\n",
      "Iter14,Testing Acc 0.9735,learning_rate 0.000487675\n",
      "Iter15,Testing Acc 0.973,learning_rate 0.00046329122\n",
      "Iter16,Testing Acc 0.9756,learning_rate 0.00044012666\n",
      "Iter17,Testing Acc 0.9772,learning_rate 0.00041812033\n",
      "Iter18,Testing Acc 0.9767,learning_rate 0.00039721432\n",
      "Iter19,Testing Acc 0.9746,learning_rate 0.0003773536\n",
      "Iter20,Testing Acc 0.9767,learning_rate 0.00035848594\n",
      "Iter21,Testing Acc 0.9782,learning_rate 0.00034056162\n",
      "Iter22,Testing Acc 0.9766,learning_rate 0.00032353355\n",
      "Iter23,Testing Acc 0.9756,learning_rate 0.00030735688\n",
      "Iter24,Testing Acc 0.9762,learning_rate 0.000291989\n",
      "Iter25,Testing Acc 0.9763,learning_rate 0.00027738957\n",
      "Iter26,Testing Acc 0.9786,learning_rate 0.0002635201\n",
      "Iter27,Testing Acc 0.979,learning_rate 0.00025034408\n",
      "Iter28,Testing Acc 0.9773,learning_rate 0.00023782688\n",
      "Iter29,Testing Acc 0.9756,learning_rate 0.00022593554\n",
      "Iter30,Testing Acc 0.9766,learning_rate 0.00021463877\n",
      "Iter31,Testing Acc 0.9763,learning_rate 0.00020390682\n",
      "Iter32,Testing Acc 0.9784,learning_rate 0.00019371149\n",
      "Iter33,Testing Acc 0.9767,learning_rate 0.0001840259\n",
      "Iter34,Testing Acc 0.977,learning_rate 0.00017482461\n",
      "Iter35,Testing Acc 0.9723,learning_rate 0.00016608338\n",
      "Iter36,Testing Acc 0.9771,learning_rate 0.00015777921\n",
      "Iter37,Testing Acc 0.9772,learning_rate 0.00014989026\n",
      "Iter38,Testing Acc 0.9789,learning_rate 0.00014239574\n",
      "Iter39,Testing Acc 0.9775,learning_rate 0.00013527596\n",
      "Iter40,Testing Acc 0.9767,learning_rate 0.00012851215\n",
      "Iter41,Testing Acc 0.9789,learning_rate 0.00012208655\n",
      "Iter42,Testing Acc 0.9788,learning_rate 0.00011598222\n",
      "Iter43,Testing Acc 0.9789,learning_rate 0.00011018311\n",
      "Iter44,Testing Acc 0.9786,learning_rate 0.000104673956\n",
      "Iter45,Testing Acc 0.9784,learning_rate 9.944026e-05\n",
      "Iter46,Testing Acc 0.9788,learning_rate 9.446825e-05\n",
      "Iter47,Testing Acc 0.9765,learning_rate 8.974483e-05\n",
      "Iter48,Testing Acc 0.9772,learning_rate 8.525759e-05\n",
      "Iter49,Testing Acc 0.9794,learning_rate 8.099471e-05\n",
      "Iter50,Testing Acc 0.9784,learning_rate 7.6944976e-05\n"
     ]
    }
   ],
   "source": [
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets(\"../../data/mnist\", one_hot=True)\n",
    "\n",
    "# 定义每个批次大小(50?, 200?)\n",
    "batch_size = 100\n",
    "\n",
    "# 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, tf.float32)\n",
    "\n",
    "# Variable和placeholder的区别：\n",
    "# variable必须初始化，主要用于存训练变量，如模型的权重，偏差。placeholder不用初始化，在session.run(xx, feed_dict)时指定，主要用于输入样本。\n",
    "# placeholder的值传进去后就不能改变；Variable可以\n",
    "\n",
    "\n",
    "# 创建一个简单的神经网络\n",
    "Weights_L1 = tf.Variable(tf.truncated_normal([784, 300], stddev=0.01))    \n",
    "biases_L1 = tf.Variable(tf.zeros([300])+0.1) \n",
    "L1 = tf.nn.tanh(tf.matmul(x, Weights_L1)+biases_L1)\n",
    "L1_dropout = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# 隐藏层L2\n",
    "Weights_L2 = tf.Variable(tf.truncated_normal([300, 300], stddev=0.01))    \n",
    "biases_L2 = tf.Variable(tf.zeros([300])+0.1) \n",
    "L2 = tf.nn.tanh(tf.matmul(L1_dropout, Weights_L2)+biases_L2)\n",
    "L2_dropout = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# 输出层L3\n",
    "Weights_L3 = tf.Variable(tf.truncated_normal([300, 10], stddev=0.01))    \n",
    "biases_L3 = tf.Variable(tf.zeros([10])+0.1) \n",
    "prediction = tf.nn.softmax(tf.matmul(L2_dropout, Weights_L3) + biases_L3)   \n",
    "\n",
    "# 交叉熵\n",
    "# loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "\n",
    "# 梯度下降法最小化代价函数（使用其他优化方式?）\n",
    "# train = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "# train = tf.train.AdadeltaOptimizer(0.001).minimize(loss)\n",
    "train = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
    "# 初始化变量 \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 存放结果在一个布尔型列表中\n",
    "# 首先让我们找出那些预测正确的标签。tf.argmax 是一个非常有用的函数，\n",
    "# 它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，\n",
    "# 比如tf.argmax(y,1)返回的是模型对于任一输入x预测到的标签值，而 tf.argmax(y_,1) 代表正确的标签，\n",
    "# 我们可以用 tf.equal 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。\n",
    "correct_prediction = tf.equal(tf.arg_max(y,1), tf.arg_max(prediction,1))\n",
    "\n",
    "# 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 执行会话\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(51):   \n",
    "        # 让学习率随着迭代的次数增加逐渐减小，更好的接近全局最小值\n",
    "        sess.run(tf.assign(lr, 0.001 * (0.95 ** epoch)))\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.0})\n",
    "        learning_rate = sess.run(lr)\n",
    "        acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob:1.0})\n",
    "        print(\"Iter\" + str(epoch) + \",Testing Acc \" + str(acc) + \",learning_rate \" + str(learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【Python 脚本报错】AttributeError:'module' has no attribute 'xxx'的解决方法\n",
    "\n",
    "原因：可能是变量名或者函数名与内置函数名冲突"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdadeltaOptimizer\n",
    "Iter0,Testing Acc 0.6595\n",
    "Iter1,Testing Acc 0.6663\n",
    "Iter2,Testing Acc 0.6721\n",
    "Iter3,Testing Acc 0.6763\n",
    "Iter4,Testing Acc 0.6779\n",
    "Iter5,Testing Acc 0.6807\n",
    "Iter6,Testing Acc 0.6856\n",
    "Iter7,Testing Acc 0.6887\n",
    "Iter8,Testing Acc 0.6917\n",
    "Iter9,Testing Acc 0.6938\n",
    "Iter10,Testing Acc 0.6956\n",
    "Iter11,Testing Acc 0.6966\n",
    "Iter12,Testing Acc 0.6993\n",
    "Iter13,Testing Acc 0.7006\n",
    "Iter14,Testing Acc 0.7012\n",
    "Iter15,Testing Acc 0.7028\n",
    "Iter16,Testing Acc 0.7033\n",
    "Iter17,Testing Acc 0.7051\n",
    "Iter18,Testing Acc 0.7066\n",
    "Iter19,Testing Acc 0.7077\n",
    "Iter20,Testing Acc 0.7081\n",
    "\n",
    "# adamOptimizer, L1,L2,L3，500 neurals, 21 eporch\n",
    "Iter0,Testing Acc 0.9302,learning_rate 0.001\n",
    "Iter1,Testing Acc 0.9427,learning_rate 0.00095\n",
    "Iter2,Testing Acc 0.9517,learning_rate 0.0009025\n",
    "Iter3,Testing Acc 0.9547,learning_rate 0.000857375\n",
    "Iter4,Testing Acc 0.9597,learning_rate 0.00081450626\n",
    "Iter5,Testing Acc 0.9638,learning_rate 0.0007737809\n",
    "Iter6,Testing Acc 0.967,learning_rate 0.0007350919\n",
    "Iter7,Testing Acc 0.9678,learning_rate 0.0006983373\n",
    "Iter8,Testing Acc 0.9691,learning_rate 0.0006634204\n",
    "Iter9,Testing Acc 0.9704,learning_rate 0.0006302494\n",
    "Iter10,Testing Acc 0.9725,learning_rate 0.0005987369\n",
    "Iter11,Testing Acc 0.9741,learning_rate 0.0005688001\n",
    "Iter12,Testing Acc 0.9711,learning_rate 0.0005403601\n",
    "Iter13,Testing Acc 0.9746,learning_rate 0.0005133421\n",
    "Iter14,Testing Acc 0.9726,learning_rate 0.000487675\n",
    "Iter15,Testing Acc 0.9758,learning_rate 0.00046329122\n",
    "Iter16,Testing Acc 0.9741,learning_rate 0.00044012666\n",
    "Iter17,Testing Acc 0.9733,learning_rate 0.00041812033\n",
    "Iter18,Testing Acc 0.9756,learning_rate 0.00039721432\n",
    "Iter19,Testing Acc 0.9743,learning_rate 0.0003773536\n",
    "Iter20,Testing Acc 0.9762,learning_rate 0.00035848594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
